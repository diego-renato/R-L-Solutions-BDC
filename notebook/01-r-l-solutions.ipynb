{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\nfrom sklearn import metrics\nfrom scipy.optimize import differential_evolution","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"active_promos = pd.read_csv(\"active_promos.csv\")\nexecuted_promos = pd.read_csv(\"executed_promos.csv\")\nclients_attr = pd.read_csv(\"clients_attributes.csv\")\nsales = pd.read_csv(\"sales.csv\", encoding = \"ISO-8859-1\")\ntest = pd.read_csv(\"test_submission.csv\")\nprint(\"The dimension from the dataset is: {}\".format(active_promos.shape))\nprint(\"The dimension from the dataset executed_promois: {}\".format(executed_promos.shape))\nprint(\"The dimension from the dataset clientes_attr is: {}\".format(clients_attr.shape))\nprint(\"The dimension from the dataset sales is: {}\".format(sales.shape))\nprint(\"The dimension from the dataset test is: {}\".format(test.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making datasets\n\nLa idea es de crear funciones que automatizen la creacion de los conjuntos de entrenamiento y de test para luego poder evaluar. \n\nLa funci칩n principal que realiza todo esto es: **make_datasets** que tiene como entrada las 4 datasets por separado:\n* active_promos_df\n* executed_promos_df\n* client_attributes_df\n* sales_df\n* test_df \n* meses_key: meses para obtener nuevas variables de la data set SALES\n* CodMes_int: el c칩digo de mes para el conjunto de test, por ejemplo 201910.\n\n y la salida son dos conjuntos de datos: train y test."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Utils\n\ndef get_id(data: pd.DataFrame) -> pd.DataFrame:\n    data[\"Id\"] = data.CodigoDC.astype(str) + data.Cliente.astype(str) + data.Marca.astype(str) + data.Cupo.astype(str)\n    data = data.set_index(\"Id\")\n    return data\n\ndef get_data_frame_target(active_promos: pd.DataFrame, executed_promos: pd.DataFrame) -> pd.DataFrame:\n    active_promos = get_id(active_promos)\n    executed_promos = get_id(executed_promos)\n\n    executed_promos[\"target\"] = 1\n    active_promos = active_promos.join(executed_promos.target).replace(np.nan, 0)\n    active_promos = active_promos.reset_index().drop(\"Id\", axis=1)\n    return active_promos\n\ndef mergue_client_attr_target_df(clients_attr: pd.DataFrame, data_frame_target: pd.DataFrame) -> pd.DataFrame:\n    clients_attr = clients_attr.set_index(\"Cliente\")\n    data_frame_target = data_frame_target.set_index(\"Cliente\")\n    data_frame_target = data_frame_target.join(clients_attr)\n    data_frame_target = data_frame_target.reset_index()\n    return data_frame_target\n\ndef get_sales_df(sales: pd.DataFrame) -> pd.DataFrame:\n    sales = sales.dropna()\n    to_dummies_sales = [\"ClaseEnvase\", \"SegmentoPrecio\"]\n    Descuento = pd.get_dummies(sales.Dcto == 0)\n    Descuento.columns = [\"ConDescuento\", \"SinDescuento\"]\n    Nr = pd.get_dummies(sales.Nr < 0)\n    Nr.columns = [\"NrPositivo\", \"NrNegativo\"]\n    ClaseEnvase = pd.get_dummies(sales.ClaseEnvase, prefix = \"ClaseEnvase\")\n    SegmentoPrecio = pd.get_dummies(sales.SegmentoPrecio.astype(int), prefix = \"SegmentoPrecio\")\n    sales = sales.join(Descuento).join(Nr).join(ClaseEnvase).join(SegmentoPrecio)\n    sales[\"MesCod\"] = sales.A침o.astype(str) + sales.Mes.map(\"{:02}\".format)\n    sales[\"MesCod\"] = sales.MesCod.astype(int)\n    sales[\"Cupo\"] = sales.Cupo.astype(int)\n    return sales.drop([\"A침o\", \"Mes\"], axis=1).drop(to_dummies_sales, axis=1)\n\ndef marca_cupo_sales_df(sales: pd.DataFrame, meses) -> pd.DataFrame:\n    sales = get_sales_df(sales)\n    df_1 = sales.groupby([\"MesCod\", \"Cliente\", \"Marca\", \"Cupo\"]).sum().sort_index().astype(\"int32\")\n    sales_complementos = []\n    for mes in meses.keys():\n        res_sum = pd.concat([\n            df_1.loc[meses[mes]].groupby([\"Cliente\", \"Marca\", \"Cupo\"]).sum()\n        ], axis=1)\n        res_sum[\"MesCod\"] = mes\n        res_sum = res_sum.reset_index().set_index([\"Cliente\", \"Marca\", \"Cupo\", \"MesCod\"]).astype(\"float32\").add_suffix('_sum')\n\n        res_min = pd.concat([\n            df_1.loc[meses[mes]].groupby([\"Cliente\", \"Marca\", \"Cupo\"]).min()\n        ], axis=1)\n        res_min[\"MesCod\"] = mes\n        res_min = res_min.reset_index().set_index([\"Cliente\", \"Marca\", \"Cupo\", \"MesCod\"]).astype(\"float32\").add_suffix('_min')\n\n        res_max = pd.concat([\n            df_1.loc[meses[mes]].groupby([\"Cliente\", \"Marca\", \"Cupo\"]).max()\n        ], axis=1)\n        res_max[\"MesCod\"] = mes\n        res_max = res_max.reset_index().set_index([\"Cliente\", \"Marca\", \"Cupo\", \"MesCod\"]).astype(\"float32\").add_suffix('_max')\n\n        sales_complementos.append(res_sum.join(res_min).join(res_max))\n\n    gc.collect()\n    sales_complementos = pd.concat(sales_complementos)\n    return sales_complementos\n\ndef personal_info_sales_df(sales: pd.DataFrame, meses) -> pd.DataFrame:\n    sales = get_sales_df(sales)\n    df_1 = sales.drop([\"Marca\", \"Cupo\"], axis = 1).groupby([\"MesCod\", \"Cliente\"]).sum().sort_index().astype(\"int32\")\n    sales_complementos = []\n    for mes in meses.keys():\n        res_sum = pd.concat([\n            df_1.loc[meses[mes]].groupby(\"Cliente\").sum()\n        ], axis=1)\n        res_sum[\"MesCod\"] = mes\n        res_sum = res_sum.reset_index().set_index([\"Cliente\", \"MesCod\"]).astype(\"float32\").add_suffix('_cliente_sum')\n\n        res_min = pd.concat([\n            df_1.loc[meses[mes]].groupby(\"Cliente\").min()\n        ], axis=1)\n        res_min[\"MesCod\"] = mes\n        res_min = res_min.reset_index().set_index([\"Cliente\", \"MesCod\"]).astype(\"float32\").add_suffix('_cliente_min')\n\n        res_max = pd.concat([\n            df_1.loc[meses[mes]].groupby(\"Cliente\").max()\n        ], axis=1)\n        res_max[\"MesCod\"] = mes\n        res_max = res_max.reset_index().set_index([\"Cliente\", \"MesCod\"]).astype(\"float32\").add_suffix('_cliente_max')\n\n        sales_complementos.append(res_sum.join(res_min).join(res_max))\n\n    gc.collect()\n    print(\"contatenando complementos\")\n    sales_complementos = pd.concat(sales_complementos)\n    return sales_complementos\n\n\ndef get_train_df(clients_attr: pd.DataFrame, active_promos: pd.DataFrame, executed_promos: pd.DataFrame\n                 , marca_cupo_info_sales: pd.DataFrame, personal_info_sales: pd.DataFrame) -> pd.DataFrame:\n    data_frame_target = get_data_frame_target(active_promos, executed_promos)\n    train = mergue_client_attr_target_df(clients_attr, data_frame_target)\n    train[\"MesCod\"] = pd.DatetimeIndex(train.Fecha_Desde).year.astype(str) + pd.DatetimeIndex(\n        train.Fecha_Desde).month.map(\"{:02}\".format).astype(str)\n    train[\"MesCod\"] = train.MesCod.astype(int)\n    train[\"Grupo\"] = 1\n    train.loc[(train.MesCod>=201811)&(train.MesCod<=201901),\"Grupo\"] = 2\n    train.loc[(train.MesCod>=201902)&(train.MesCod<=201904),\"Grupo\"] = 3\n    train.loc[(train.MesCod>=201908)&(train.MesCod<=201909),\"Grupo\"] = 4\n    train = train.set_index([\"Cliente\", \"Marca\", \"Cupo\", \"MesCod\"]).join(\n        marca_cupo_info_sales).reset_index().set_index([\"Cliente\", \"MesCod\"]).join(personal_info_sales).reset_index()\n    print(\"Dataset train terminada.\")\n    return train\n\n\ndef get_test_df(test: pd.DataFrame, clients_attr: pd.DataFrame, marca_cupo_info_sales: pd.DataFrame,\n                personal_info_sales: pd.DataFrame, MesCod: int) -> pd.DataFrame:\n    test[\"MesCod\"] = MesCod\n    test = mergue_client_attr_target_df(clients_attr, test)\n    test[\"Grupo\"] = 3\n    test = test.set_index([\"Cliente\", \"Marca\", \"Cupo\", \"MesCod\"]).join(marca_cupo_info_sales).reset_index().set_index(\n        [\"Cliente\", \"MesCod\"]).join(personal_info_sales).reset_index()\n    ano = str(MesCod)[0:4]\n    mes = str(MesCod)[4:]\n    test[\"Fecha_Hasta\"] = pd.to_datetime(str(mes+\"/01/\"+ano))\n    print(\"Dataset test terminada.\")\n    return test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_datasets_initial(active_promos_df, executed_promos_df, client_attributes_df, sales_df, test_df, meses_key, CodMes_int):\n    marca_cupo_sales = marca_cupo_sales_df(sales_df, meses_key)\n    personal_info_sales = personal_info_sales_df(sales_df, meses_key)\n    data_train = get_train_df(client_attributes_df, active_promos_df, executed_promos_df\n                              , marca_cupo_sales, personal_info_sales)\n    data_test = get_test_df(test_df, client_attributes_df, marca_cupo_sales, personal_info_sales, CodMes_int)\n    return data_train, data_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data_final_features(data_final: pd.DataFrame) -> pd.DataFrame:\n    to_dummies_clients_attr = [\"Region\", \"Gerencia\", \"SubCanal\", \"Estrato\", \"Mes\"]\n    missing_columns = [\"Nr_sum\", \"Nr_cliente_sum\"]\n    for column_i in missing_columns:\n        data_final[column_i+\"_not_na\"] = (data_final[column_i].isna()== False).astype(int) \n    \n    data_final[\"Mes\"] = pd.DatetimeIndex(data_final.FechaAltaCliente).month\n    data_final[\"AnosTranscurridos\"] =pd.DatetimeIndex(data_final.Fecha_Hasta).year - pd.DatetimeIndex(data_final.FechaAltaCliente).year\n    data_final[\"MesesTranscurridos\"] = ( (pd.DatetimeIndex(data_final.Fecha_Hasta) - pd.DatetimeIndex(data_final.FechaAltaCliente))/ np.timedelta64(1, 'D') -\n             (pd.DatetimeIndex(data_final.Fecha_Hasta).day+ pd.DatetimeIndex(data_final.FechaAltaCliente).day) )/30\n    data_final.TipoPoblacion = data_final.TipoPoblacion.astype(int) - 1\n    \n    Region_dummies = pd.get_dummies(data_final.Region,prefix = \"Region\")\n    Gerencia_dummies = pd.get_dummies(data_final.Gerencia,prefix = \"Gerencia\")\n    Canales_dummies = pd.get_dummies(data_final.SubCanal,prefix = \"Canales\")\n    Estrato_dummies = pd.get_dummies(data_final.Estrato,prefix = \"Estrato\")\n    Mes_dummies = pd.get_dummies(data_final.Mes,prefix = \"Mes\")\n    Marca_dummies = pd.get_dummies(data_final.Marca,prefix = \"Marca\")\n    \n    data_final = data_final.join(Region_dummies).join(Gerencia_dummies).join(Canales_dummies).join(Estrato_dummies).join(Mes_dummies).join(Marca_dummies).drop(to_dummies_clients_attr,axis = 1)\n    \n    return data_final\n\n\ndef reduce_by_correlation(train_df: pd.DataFrame, cut_off_0_1) -> pd.DataFrame:\n    correlated_features = set()\n    non = [\"Cliente\", 'MesCod', 'Marca', 'Cupo', 'target',\"Grupo\"]\n    train_df = train_df.drop(non,axis=1) \n    correlation_matrix = train_df.dropna().corr()\n    for i in range(len(correlation_matrix.columns)):\n         for j in range(i):\n                if abs(correlation_matrix.iloc[i, j]) > cut_off_0_1:\n                    colname = correlation_matrix.columns[i]\n                    correlated_features.add(colname)\n    correlated_features=list(correlated_features)\n    return correlated_features\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_datasets_final(train: pd.DataFrame, test: pd.DataFrame, var_cutoff, cor_cutoff) -> pd.DataFrame:\n    train = get_data_final_features(train).drop([\"FechaAltaCliente\",\"Fecha_Hasta\",\"Fecha_Desde\",\"CodigoDC\"] ,axis= 1)\n    test = get_data_final_features(test).drop([\"FechaAltaCliente\",\"Fecha_Hasta\"] ,axis= 1)\n    test = test.rename(columns = {'Ejecuto_Promo': 'target'}, inplace = False)\n    train = train.loc[:,test.columns] #obtener mismas variables\n    \n    to_drop_correlation_high = reduce_by_correlation(train, cor_cutoff)\n    train = train.drop(to_drop_correlation_high, axis = 1)\n    test = test.drop(to_drop_correlation_high, axis = 1)\n    \n    not_incluide= [\"Grupo\", \"target\", \"MesCod\"]\n    train_df_var = train.drop(not_incluide, axis = 1)\n    to_drop_var_little = train_df_var.loc[:,train_df_var.var() < var_cutoff].columns.to_numpy()\n    train = train.drop(to_drop_var_little, axis = 1)\n    test = test.drop(to_drop_var_little, axis = 1)\n    return train, test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### To submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_submission_df(test: pd.DataFrame, test_probs: pd.DataFrame):\n    test_submit = test.copy()\n    test_submit[\"Ejecuto_Promo\"] = test_probs.loc[test_submit.index]\n    submission_df = test_submit[[\"Cliente\", \"Marca\", \"Cupo\", \"Ejecuto_Promo\"]].copy()\n    submission_df.to_csv('submission.csv',index=None,encoding='utf-8')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1><center><font size=\"6\">Datasets creation</font></center></h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"meses = {\n    201806: slice(201801, 201804),\n    201807: slice(201802, 201805),\n    201808: slice(201803, 201806),\n    201809: slice(201804, 201807),\n    201810: slice(201805, 201808),\n    201811: slice(201806, 201809),\n    201812: slice(201807, 201810),\n    201901: slice(201808, 201811),\n    201902: slice(201809, 201812),\n    201903: slice(201810, 201901),\n    201904: slice(201811, 201902),\n    201905: slice(201812, 201903),\n    201906: slice(201901, 201904),\n    201907: slice(201902, 201905),\n    201908: slice(201903, 201906),\n    201909: slice(201904, 201907),\n    201910: slice(201905, 201908)\n}\nCodMes = 201910\ntrain_diego_inicial, test_diego_inicial = make_datasets_initial(active_promos, executed_promos, \n                            clients_attr, sales, test, meses, CodMes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_cutoff = 0\ntrain_diego, test_diego = make_datasets_final(train_diego_inicial, test_diego_inicial, var_cutoff, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meses = {\n    201806: slice(201801, 201804),\n    201807: slice(201801, 201805),\n    201808: slice(201801, 201806),\n    201809: slice(201801, 201807),\n    201810: slice(201801, 201808),\n    201811: slice(201801, 201809),\n    201812: slice(201801, 201810),\n    201901: slice(201801, 201811),\n    201902: slice(201801, 201812),\n    201903: slice(201801, 201901),\n    201904: slice(201801, 201902),\n    201905: slice(201801, 201903),\n    201906: slice(201801, 201904),\n    201907: slice(201801, 201905),\n    201908: slice(201801, 201906),\n    201909: slice(201801, 201907),\n    201910: slice(201801, 201908)\n}\nCodMes = 201910\ntrain_paolo_inicial, test_paolo_inicial = make_datasets_initial(active_promos, executed_promos, \n                            clients_attr, sales, test, meses, CodMes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_cutoff = 0\ntrain_paolo, test_paolo = make_datasets_final(train_paolo_inicial, test_paolo_inicial, var_cutoff, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# x_train, y_train particition"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_diego = train_diego.reset_index()[[\"index\",\"target\"]].copy().set_index(\"index\")\nx_train_diego  = train_diego.drop(\"target\",axis=1)\n\ny_train_paolo = train_paolo.reset_index()[[\"index\",\"target\"]].copy().set_index(\"index\")\nx_train_paolo  = train_paolo.drop(\"target\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1><center><font size=\"6\">Datasets Modeling using LGBM</font></center></h1>\n\nPresentamos las soluciones despues de optimizar algunos hyperparametros mediante group cross validation."},{"metadata":{},"cell_type":"markdown","source":"# LightGBM modelo Diego "},{"metadata":{"trusted":true},"cell_type":"code","source":"result = [0.04,7,9,174,5000]\nlr = result[0]\nmax_depth = int(result[1])\nlambda_l1 = result[2]\nnum_itera = int(result[3])\nmin_data_in_leaf = int(result[4])\ngroup_kfold = GroupKFold(n_splits=4)\ntrain_diego_probsLGBM = []\ntest_diego_probsLGBM = []\n\nfor i,(a,b) in enumerate(group_kfold.split(x_train_diego, y_train_diego.loc[x_train_diego.index], x_train_diego.MesCod)):\n    Xt = x_train_diego.loc[a,:].drop(\"Grupo\", axis = 1)\n    yt = y_train_diego.loc[Xt.index, \"target\"]\n\n    Xv = x_train_diego.loc[b,:].drop(\"Grupo\", axis = 1)\n    yv = y_train_diego.loc[Xv.index, \"target\"]\n\n    learner = LGBMClassifier(learning_rate=lr,max_depth=max_depth,num_iterations=num_itera,\n                                    lambda_l1=lambda_l1, min_data_in_leaf = min_data_in_leaf,\n                            seed = 1997)\n\n    learner.fit(Xt, yt  , eval_metric=\"auc\",eval_set= [(Xt, yt),(Xv, yv)], verbose=174)\n\n\n    train_diego_probsLGBM.append(pd.Series(learner.predict_proba(Xv)[:, -1],\n                                    index=Xv.index, name=\"probs\"+ str(i)))\n    test_diego_probsLGBM.append(pd.Series(learner.predict_proba(test_diego.drop([\"target\",\"Grupo\"],axis=1))[:, -1],\n                                    index=test_diego.index, name=\"probs\"+ str(i)))\n        \ntrain_diego_probsLGBM = pd.concat(train_diego_probsLGBM, axis=1).mean(axis=1)\ntest_diego_probsLGBM = pd.concat(test_diego_probsLGBM, axis=1).mean(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = [0.03,4,7.65,210,820]\nlr = result[0]\nmax_depth = int(result[1])\nlambda_l1 = result[2]\nnum_itera = int(result[3])\nmin_data_in_leaf = int(result[4])\ngroup_kfold = GroupKFold(n_splits=4)\ntrain_paolo_probsLGBM = []\ntest_paolo_probsLGBM = []\n\nfor i,(a,b) in enumerate(group_kfold.split(x_train_paolo, y_train_paolo.loc[x_train_paolo.index], x_train_paolo.MesCod)):\n    Xt = x_train_paolo.loc[a,:].drop(\"Grupo\", axis = 1)\n    yt = y_train_paolo.loc[Xt.index, \"target\"]\n\n    Xv = x_train_paolo.loc[b,:].drop(\"Grupo\", axis = 1)\n    yv = y_train_paolo.loc[Xv.index, \"target\"]\n\n    learner = LGBMClassifier(learning_rate=lr,max_depth=max_depth,num_iterations=num_itera,\n                                    lambda_l1=lambda_l1, min_data_in_leaf = min_data_in_leaf,\n                            seed = 1997, is_imbalanced = True)\n\n    learner.fit(Xt, yt  , eval_metric=\"auc\",eval_set= [(Xt, yt),(Xv, yv)], verbose=210)\n\n\n    train_paolo_probsLGBM.append(pd.Series(learner.predict_proba(Xv)[:, -1],\n                                    index=Xv.index, name=\"probs\"+ str(i)))\n    test_paolo_probsLGBM.append(pd.Series(learner.predict_proba(test_paolo.drop([\"target\",\"Grupo\"],axis=1))[:, -1],\n                                    index=test_paolo.index, name=\"probs\"+ str(i)))\n        \ntrain_paolo_probsLGBM = pd.concat(train_paolo_probsLGBM, axis=1).mean(axis=1)\ntest_paolo_probsLGBM = pd.concat(test_paolo_probsLGBM, axis=1).mean(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUC_cv_mean = metrics.roc_auc_score(y_train_diego.loc[train_diego_probsLGBM.index,], \n                               np.mean([train_paolo_probsLGBM,train_diego_probsLGBM], axis=0))\nAUC_cv_max = metrics.roc_auc_score(y_train_diego.loc[train_diego_probsLGBM.index,], \n                               np.max([train_paolo_probsLGBM,train_diego_probsLGBM], axis=0))\nAUC_cv_min = metrics.roc_auc_score(y_train_diego.loc[train_diego_probsLGBM.index,], \n                               np.min([train_paolo_probsLGBM,train_diego_probsLGBM], axis=0))\nprint(\"AUC_cv_mean:\", AUC_cv_mean)\nprint(\"AUC_cv_max:\", AUC_cv_max)\nprint(\"AUC_cv_min:\", AUC_cv_min)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.roc_auc_score(y_train_diego.loc[train_diego_probsLGBM.index,], \n                                           train_diego_probsLGBM)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_probsLGBM = np.min([test_paolo_probsLGBM,test_diego_probsLGBM], axis=0)\ntest_probsLGBM = pd.Series(test_probsLGBM,index=test_paolo_probsLGBM.index)\nget_submission_df(test_diego,test_probsLGBM)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}